{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2ccdf4-0f5e-45f2-b40d-f2a6d5184d1a",
   "metadata": {
    "name": "Title",
    "collapsed": false,
    "resultHeight": 151
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex Search"
  },
  {
   "cell_type": "markdown",
   "id": "5548b002-a078-4731-af05-116ad4d1d87e",
   "metadata": {
    "name": "Step_1",
    "collapsed": false,
    "resultHeight": 112
   },
   "source": "# Step 1: Organize Documents and Create Pre-Processing Function"
  },
  {
   "cell_type": "markdown",
   "id": "61243478-487f-48aa-934c-8c445d72a3e1",
   "metadata": {
    "name": "Sample_Document_Download",
    "collapsed": false,
    "resultHeight": 868
   },
   "source": "## Sample Document Download\n\n### Overview\nThis step provides sample documents for testing the RAG system. The documents include:\n- Bike product documentation\n- Ski equipment manuals\n- User guides and specifications\n- Product information sheets\n\n### Available Documents\n**Bike Documentation**\n- Mondracer Infant Bike\n- Premium Bycycle User Guide\n- The Ultimate Downhill Bike\n- The Xtreme Road Bike 105 SL\n\n**Ski Equipment**\n- Ski Boots TDBootz Special\n- Carver Skis Specification Guide\n- OutPiste Skis Specification Guide\n- RacingFast Skis Specification Guide\n\n### Important Notes\n- Documents contain specific product information\n- Files are text-based PDFs\n- Content is fictional for demonstration\n- Additional documents can be added\n- System will be tested with and without document context"
  },
  {
   "cell_type": "markdown",
   "id": "cbbe3981-e91f-441f-b2f2-d5fd72aab5a4",
   "metadata": {
    "name": "Database_and_Schema_Setup",
    "collapsed": false,
    "resultHeight": 630
   },
   "source": "## Database and Schema Setup\n\n### Overview\nThis section establishes the foundational database structure for implementing a RAG (Retrieval Augmented Generation) system using Snowflake Cortex Search. The setup creates a dedicated database and schema for storing:\n- Document chunks and embeddings\n- Search functionality\n- Processing functions\n- Application components\n\n### Architecture\nThe database structure follows a simple organization:\n1. **Database Layer** - CC_QUICKSTART_CORTEX_SEARCH_DOCS for all RAG components\n2. **Schema Layer** - DATA schema for organizing objects and tables\n3. **Objects** - Will contain:\n   - Document storage tables\n   - Processing functions\n   - Search services\n   - Application components\n\n"
  },
  {
   "cell_type": "code",
   "id": "25c81ab6-c78b-4a70-bd1b-e851e902ec97",
   "metadata": {
    "language": "sql",
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "use role DEMOADMIN;\n\nCREATE DATABASE CC_QUICKSTART_CORTEX_SEARCH_DOCS;\nCREATE SCHEMA DATA;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "745e096e-bffd-4883-bb95-86000b177b75",
   "metadata": {
    "name": "Text_Chunking_Function",
    "collapsed": false,
    "resultHeight": 595
   },
   "source": "## Text Chunking Function\n\n### Overview\nThis UDF (User-Defined Function) implements document text chunking using LangChain's RecursiveCharacterTextSplitter. The function:\n- Takes a text string as input\n- Splits it into overlapping chunks\n- Returns chunks as a table of strings\n\n### Technical Details\n- **Runtime**: Python 3.9\n- **Dependencies**: \n  - snowflake-snowpark-python\n  - langchain\n- **Input**: Single text string\n- **Output**: Table of text chunks\n- **Chunk Parameters**:\n  - Size: 1512 characters\n  - Overlap: 256 characters"
  },
  {
   "cell_type": "code",
   "id": "1c1a05e7-00d6-4da6-bfb5-ff00c1995945",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "create or replace function text_chunker(pdf_text string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'text_chunker'\npackages = ('snowflake-snowpark-python', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport pandas as pd\n\nclass text_chunker:\n\n    def process(self, pdf_text: str):\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 1512, #Adjust this as you see fit\n            chunk_overlap  = 256, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(pdf_text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61bb1dcf-960a-4f2d-9438-c39f719a0043",
   "metadata": {
    "name": "Internal_Stage_Creation",
    "collapsed": false,
    "resultHeight": 451
   },
   "source": "## Internal Stage Creation\n\n### Overview\nThis SQL creates a named internal stage for storing document files with server-side encryption and directory table functionality enabled. The stage will be used to:\n- Store documents securely within Snowflake\n- Track and manage staged files using directory tables\n- Support the document processing pipeline\n\n### Technical Specifications\n- **Stage Type**: Internal (Snowflake-managed)\n- **Encryption**: Server-side encryption (SNOWFLAKE_SSE)\n- **Directory Table**: Enabled\n- **Purpose**: Document storage for RAG implementation"
  },
  {
   "cell_type": "code",
   "id": "c796ce84-eee5-4e6d-a45b-8a6293fd05e8",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4cd5760-86a6-4457-99ee-ca25a4291c88",
   "metadata": {
    "name": "Document_Upload_to_Stage",
    "collapsed": false,
    "resultHeight": 856
   },
   "source": "## Document Upload to Stage\n\n### Overview\nThis manual step involves uploading PDF documents to the previously created internal stage using Snowflake's web interface (Snowsight). The process enables:\n- Document storage in the secure stage\n- Preparation for text extraction\n- Integration with the RAG pipeline\n\n### Navigation Steps\n1. **Access Staging Area**\n   - Open Snowsight interface\n   - Navigate to Data browser\n   - Select database: CC_QUICKSTART_CORTEX_SEARCH_DOCS\n   - Select schema: DATA\n   - Open Stages section\n   - Select DOCS stage\n\n2. **Upload Process**\n   - Click '+Files' button (top right)\n   - Drag and drop PDF files\n   - Wait for upload completion\n\n### Important Notes\n- Ensure PDFs are properly formatted and readable\n- Files will be encrypted using SNOWFLAKE_SSE\n- Directory table will automatically track uploaded files\n- Files become available for subsequent processing steps"
  },
  {
   "cell_type": "markdown",
   "id": "f1ea024a-c347-4e4a-a942-8c5a990849a8",
   "metadata": {
    "name": "Stage_Content_Verification",
    "collapsed": false,
    "resultHeight": 537
   },
   "source": "## Stage Content Verification\n\n### Overview\nThis command lists all files currently stored in the DOCS stage, allowing verification of successful document uploads. The command helps:\n- Confirm file upload status\n- View file metadata\n- Check stage contents\n\n### Technical Details\n- **Command**: Native Snowflake LS command\n- **Target**: DOCS internal stage\n- **Output**: List of files with metadata including:\n  - File names\n  - Sizes\n  - Last modified timestamps\n  - Status information"
  },
  {
   "cell_type": "code",
   "id": "d1d73288-ce41-4eb3-bbb4-e330e9c00280",
   "metadata": {
    "language": "sql",
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "ls @docs;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5baf2b69-7ff9-48f5-a4be-22937b9e30bb",
   "metadata": {
    "name": "Step_2",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Step 2: Pre-process and Label Documents"
  },
  {
   "cell_type": "markdown",
   "id": "21db2386-8fea-4ca9-9063-1262bc810c05",
   "metadata": {
    "name": "cell25",
    "collapsed": false,
    "resultHeight": 577
   },
   "source": "## Document Chunks Storage Table\n\n### Overview\nThis table definition creates the primary storage structure for processed document chunks and their metadata. The table serves as:\n- Central repository for document fragments\n- Storage for document metadata\n- Reference point for search operations\n\n### Schema Details\nThe table includes fields for:\n- Document location tracking\n- File metadata storage\n- Text chunk storage\n- Document categorization\n\n### Technical Specifications\nAll VARCHAR fields use maximum size (16777216) to accommodate varying content lengths."
  },
  {
   "cell_type": "code",
   "id": "61025a66-8a9c-49b5-836d-2ca7e7af9150",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "create or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216), -- Piece of text\n    CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb7451d6-ce81-47a1-b41e-ad10a8631112",
   "metadata": {
    "name": "cell26",
    "collapsed": false,
    "resultHeight": 521
   },
   "source": "## Document Processing Pipeline\n\n### Overview\nThis SQL statement implements the core document processing pipeline, combining:\n- Document metadata extraction\n- PDF text parsing\n- Text chunking\n- Storage of processed chunks\n\n### Technical Details\nThe pipeline performs these operations:\n- Reads document metadata from stage directory\n- Parses PDF content using Cortex\n- Chunks text using custom function\n- Generates scoped URLs for access\n- Stores results in chunks table\n"
  },
  {
   "cell_type": "code",
   "id": "8a4202b7-c79e-47f2-9fdb-ccac3ca50687",
   "metadata": {
    "language": "sql",
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        directory(@docs),\n        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, \n                              relative_path, {'mode': 'LAYOUT'})))) as func;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34cf6887-642a-42c0-be95-4c0a142f2dc6",
   "metadata": {
    "name": "cell27",
    "collapsed": false,
    "resultHeight": 464
   },
   "source": "## Document Category Classification\n\n### Overview\nThis SQL creates a temporary table that categorizes documents using LLM-based classification. The process:\n- Identifies unique documents\n- Uses Llama 3 70B to classify content\n- Stores classifications for later use\n\n### Technical Details\nThe query structure includes:\n- CTE for unique document identification\n- LLM integration using Cortex COMPLETE\n- Single-word category assignment\n- Temporary table storage"
  },
  {
   "cell_type": "code",
   "id": "dfbf980d-dea2-4645-840a-cd0b481a6847",
   "metadata": {
    "language": "sql",
    "name": "cell8",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE\nOR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n  SELECT\n    DISTINCT relative_path\n  FROM\n    docs_chunks_table\n),\ndocs_category_cte AS (\n  SELECT\n    relative_path,\n    TRIM(snowflake.cortex.COMPLETE (\n      'llama3-70b',\n      'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'\n    ), '\\n') AS category\n  FROM\n    unique_documents\n)\nSELECT\n  *\nFROM\n  docs_category_cte;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c97dbda-8a6b-451d-a7e8-71249ad725e4",
   "metadata": {
    "name": "cell28",
    "collapsed": false,
    "resultHeight": 219
   },
   "source": "## Category Distribution Analysis\n\n### Overview\nThis query retrieves the unique document categories assigned by the LLM classification process. The query:\n- Aggregates category assignments\n- Eliminates duplicate categories\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "f977528a-80a6-4eab-b5c8-d4165f8d6d55",
   "metadata": {
    "language": "sql",
    "name": "cell9",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "select category from docs_categories group by category;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8632cc71-fb36-4361-8f2d-fcfdfc11d70c",
   "metadata": {
    "name": "cell29",
    "collapsed": false,
    "resultHeight": 219
   },
   "source": "## Category Assignment Review\n\n### Overview\nThis query displays the complete category classification results, showing:\n- Document paths\n- Assigned categories"
  },
  {
   "cell_type": "code",
   "id": "25abf1aa-c247-41eb-81c6-672fdfb95a1f",
   "metadata": {
    "language": "sql",
    "name": "cell10",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "select * from docs_categories;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21c5e485-1ef1-4fb6-ac44-805f72e85c40",
   "metadata": {
    "name": "cell30",
    "collapsed": false,
    "resultHeight": 451
   },
   "source": "## Document Category Assignment\n\n### Overview\nThis SQL statement updates the document chunks table with category classifications from the temporary categories table. The update:\n- Assigns categories to all chunks\n- Maintains document-level categorization\n- Enables category-based filtering\n\n### Technical Details\n- **Target**: docs_chunks_table\n- **Source**: docs_categories temporary table\n- **Join Condition**: relative_path matching\n- **Updated Field**: category column"
  },
  {
   "cell_type": "code",
   "id": "9ae67d1c-5e98-42cc-8113-c82593ca2527",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "update docs_chunks_table \n  SET category = docs_categories.category\n  from docs_categories\n  where  docs_chunks_table.relative_path = docs_categories.relative_path;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a26cb06c-c94c-4678-9e10-dcb5f07dbf8f",
   "metadata": {
    "name": "Step_3",
    "collapsed": false,
    "resultHeight": 496
   },
   "source": "# Step 3: Cortex Search Service Creation\n\n### Overview\nThis SQL creates a Cortex Search Service for semantic document search. The service:\n- Enables natural language querying of document chunks\n- Provides category-based filtering\n- Maintains document metadata access\n- Updates index frequently\n\n### Technical Specifications\n- **Search Column**: chunk (document text segments)\n- **Filter Attribute**: category\n- **Compute**: COMPUTE_WH warehouse\n- **Update Frequency**: 1-minute target lag\n- **Metadata**: Preserves file paths and URLs"
  },
  {
   "cell_type": "code",
   "id": "a1d79ba4-add5-44e7-8d20-c89c60844d88",
   "metadata": {
    "language": "sql",
    "name": "cell12",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\nON chunk\nATTRIBUTES category\nwarehouse = COMPUTE_WH\nTARGET_LAG = '1 minute'\nas (\n    select chunk,\n        relative_path,\n        file_url,\n        category\n    from docs_chunks_table\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bceee172-c012-492b-82de-d4a85e65b1fb",
   "metadata": {
    "name": "Step_4",
    "collapsed": false,
    "resultHeight": 1092
   },
   "source": "# Step 4: Build Chat UI with Retrieval and Generation Logic\n## Streamlit Chat Interface Setup\n\n### Overview\nThis step creates a Streamlit application that provides a user interface for the RAG system. The application:\n- Enables natural language queries against documents\n- Demonstrates RAG vs. pure LLM responses\n- Displays retrieved context chunks\n- Maintains security through Snowflake's access controls\n\n### Technical Requirements\n- **Platform**: Streamlit in Snowflake\n- **Database**: CC_QUICKSTART_CORTEX_SEARCH_DOCS\n- **Schema**: DATA\n- **Warehouse**: Small compute warehouse\n- **App Name**: CC_CORTEX_SEARCH_APP\n\n### Implementation Steps\n1. **Access Streamlit**\n   - Navigate to Streamlit tab in left panel\n   - Click + Streamlit App button\n\n2. **Configure Application**\n   - Set application name\n   - Select compute warehouse\n   - Choose database and schema\n   - Configure access settings\n\n### Security Notes\n- Application runs within Snowflake environment\n- Access controlled by Snowflake RBAC\n- Data remains within Snowflake security boundary\n- Only authorized users can access documents"
  },
  {
   "cell_type": "markdown",
   "id": "90877d0a-b3ce-4745-a0ce-7c83b20c2f8e",
   "metadata": {
    "name": "Step_5",
    "collapsed": false,
    "resultHeight": 1101
   },
   "source": "# Step 5: Build a ChatBot UI with Conversation History\n## Conversational ChatBot Implementation\n\n### Overview\nThis section extends the basic RAG interface into a full conversational chatbot that maintains context across interactions. The implementation:\n- Creates a chat-style interface\n- Maintains conversation history\n- Uses sliding window for context management\n- Implements conversation summarization\n- Leverages Snowflake Cortex via Python API\n\n### Technical Requirements\n**Dependencies**\n- snowflake-ml-python (1.6.2)\n- snowflake.core (0.9.0)\n- python (3.8)\n- snowflake-snowpark-python (1.22.1)\n- streamlit (1.26.0)\n\n### Key Features\n- **Stateful Conversations**: Maintains chat history despite LLM statelessness\n- **Context Window Management**: Uses sliding window technique\n- **Dynamic Context Retrieval**: Combines conversation summary with current query\n- **Streamlit Chat Elements**: Enhanced UI components\n- **Python API Integration**: Direct access to Snowflake Cortex\n\n### Architecture Notes\n- Implements conversation memory management\n- Uses summarization for context preservation\n- Combines historical context with new queries\n- Manages context window limitations\n- Provides seamless chat experience"
  },
  {
   "cell_type": "code",
   "id": "2aebdb57-6081-44d4-813b-2f81880b527b",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT \n    CURRENT_WAREHOUSE(),\n    CURRENT_DATABASE(),\n    CURRENT_SCHEMA(),\n    CURRENT_ROLE();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f017a8bb-74e2-4da2-b408-e6fbcf389ff0",
   "metadata": {
    "name": "Step_6",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Step 6: Automatic Processing of New Documents"
  },
  {
   "cell_type": "markdown",
   "id": "b48c0be8-29de-4c9a-925d-afa9b4dbfa83",
   "metadata": {
    "name": "cell32",
    "collapsed": false,
    "resultHeight": 454
   },
   "source": "## Stream Setup for Document Changes\n\n### Overview\nThis SQL sequence sets up a stream to track changes in the document stage. The setup:\n- Establishes correct database context\n- Creates change tracking for staged files\n- Enables automated processing of new documents\n\n### Technical Details\n- **Database**: CC_QUICKSTART_CORTEX_SEARCH_DOCS\n- **Schema**: DATA\n- **Stream Type**: Stage-based\n- **Monitored Object**: DOCS stage\n- **Purpose**: Change data capture (CDC) for documents"
  },
  {
   "cell_type": "code",
   "id": "1872d2e9-644f-4933-b99d-393b2dafa5b7",
   "metadata": {
    "language": "sql",
    "name": "cell13",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "USE DATABASE CC_QUICKSTART_CORTEX_SEARCH_DOCS;\nUSE SCHEMA DATA;\ncreate or replace stream docs_stream on stage docs;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c06841d2-b034-48e9-a3ab-cac458c1b545",
   "metadata": {
    "name": "cell33",
    "collapsed": false,
    "resultHeight": 595
   },
   "source": "## Automated Document Processing Task\n\n### Overview\nThis SQL creates and activates a scheduled task that automatically processes new documents from the staging area. The task:\n- Monitors document stream for changes\n- Processes new PDFs when detected\n- Chunks and stores document content\n- Runs on a frequent schedule\n\n### Technical Specifications\n- **Warehouse**: COMPUTE_WH\n- **Schedule**: Every 1 minute\n- **Trigger**: Stream data presence check\n- **Operations**: \n  - PDF parsing\n  - Text chunking\n  - Metadata extraction\n  - Data insertion"
  },
  {
   "cell_type": "code",
   "id": "557e4aaf-9f91-4dc0-b635-f214660a4130",
   "metadata": {
    "language": "sql",
    "name": "cell14",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "create or replace task parse_and_insert_pdf_task \n    warehouse = COMPUTE_WH\n    schedule = '1 minute'\n    when system$stream_has_data('docs_stream')\n    as\n  \n    insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        docs_stream,\n        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})))) as func;\n\nalter task parse_and_insert_pdf_task resume;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc918e0a-7463-4b67-8715-1f328f5fc26a",
   "metadata": {
    "name": "cell34",
    "collapsed": false,
    "resultHeight": 425
   },
   "source": "## Stream Status Check\n\n### Overview\nThis query monitors the document processing stream for new files awaiting processing. The query:\n- Shows documents in transit\n- Displays file metadata\n- Helps verify processing status\n\n### Technical Details\n- **Source**: docs_stream\n- **Content**: Only shows unprocessed files\n- **Empty Results**: Indicate completed processing\n- **Purpose**: Processing pipeline monitoring"
  },
  {
   "cell_type": "code",
   "id": "4455db44-1b5b-4791-8dd2-1c958663d5a5",
   "metadata": {
    "language": "sql",
    "name": "cell18",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "select * from docs_stream;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f56a90a-6865-4ea5-9811-65253844e120",
   "metadata": {
    "name": "cell35",
    "collapsed": false,
    "resultHeight": 454
   },
   "source": "## Task Suspension Command\n\n### Overview\nThis SQL suspends the automated document processing task. The suspension:\n- Stops scheduled execution\n- Allows current executions to complete\n- Prevents processing of new documents\n- Conserves compute resources\n\n### Technical Details\n- **Target**: parse_and_insert_pdf_task\n- **Effect**: Changes task state to 'Suspended'\n- **Current Runs**: Will complete execution\n- **Scheduled Runs**: Will be cancelled"
  },
  {
   "cell_type": "code",
   "id": "6140eb55-bac9-4495-abb9-676f6c00cdf9",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "alter task parse_and_insert_pdf_task suspend;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09ab40af-9b1c-40e8-b424-86cdc3e02fe4",
   "metadata": {
    "name": "Cleanup",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Cleanup"
  },
  {
   "cell_type": "code",
   "id": "8cf3e86d-98dc-4dd7-b6a0-8cda18ec8082",
   "metadata": {
    "language": "sql",
    "name": "cell40",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SELECT \n    CURRENT_WAREHOUSE(),\n    CURRENT_DATABASE(),\n    CURRENT_SCHEMA(),\n    CURRENT_ROLE();",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73ec15a4-a036-4041-8b60-ed8bb74707fc",
   "metadata": {
    "language": "sql",
    "name": "cell41",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "USE DATABASE CC_QUICKSTART_CORTEX_SEARCH_DOCS;\nUSE SCHEMA DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "375482dd-9c11-414e-98a3-2397a6b6c9a2",
   "metadata": {
    "language": "sql",
    "name": "cell37",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "-- Suspend and drop the task\nALTER TASK parse_and_insert_pdf_task SUSPEND;\nDROP TASK IF EXISTS parse_and_insert_pdf_task;\n\n-- Drop the stream\nDROP STREAM IF EXISTS docs_stream;\n\n-- Drop the search service\nDROP CORTEX SEARCH SERVICE IF EXISTS CC_SEARCH_SERVICE_CS;\n\n\n-- Drop the tables\nDROP TABLE IF EXISTS docs_chunks_table;\nDROP TABLE IF EXISTS docs_categories;\n\n-- Drop the stage\nDROP STAGE IF EXISTS docs;\n\n-- Drop the function\nDROP FUNCTION IF EXISTS text_chunker(STRING);\n\n\n\n\n-- Drop the schema\nDROP SCHEMA IF EXISTS DATA;\n\n-- Drop the database\nDROP DATABASE IF EXISTS CC_QUICKSTART_CORTEX_SEARCH_DOCS;",
   "execution_count": null
  }
 ]
}